{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6595eef4e830ede3654b6cca00c5069f69cfa909"
   },
   "source": [
    "**This kernel contains the main workflow of a typical kaggle competition:**\n",
    "1. feature engineering\n",
    "2. intermediate data saving and reusing\n",
    "3. modeling\n",
    "4. hyper parameter tuning with grid search\n",
    "5. ensemble\n",
    "6. submission.csv generation\n",
    " \n",
    "Most of the code was copied from [Stacked Regressions to predict House Prices](http://https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard), and is refactored for better readability and reusability. Thanks Serigne!\n",
    "\n",
    "Some new stuff was added, including using hdf5 for intermediate data saving and reusing, and hyper parameter tuning with grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'data_description.txt', 'train.csv']\n",
      "all data size: (2917, 220)\n",
      "['test.h5', 'train.h5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "#test['Id'] is needed when submitting the result\n",
    "test_ID = test['Id']\n",
    "#Now drop the 'Id' colum since it's unnecessary for the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "#drop rows which are outliers\n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "#use log1p i.e. log(x+1) to transform SalePrice to a normal distribution\n",
    "#don't forget to use expm1 i.e. exp(x-1) to transform back when submitting the result \n",
    "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "y_train = train['SalePrice'].values\n",
    "ntrain = train.shape[0]\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "#Imputing missing values\n",
    "\n",
    "#PoolQC : data description says NA means \"No Pool\". \n",
    "#That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n",
    "\n",
    "#MiscFeature : data description says NA means \"no misc feature\"\n",
    "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n",
    "\n",
    "#Alley : data description says NA means \"no alley access\"\n",
    "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n",
    "\n",
    "#Fence : data description says NA means \"no fence\"\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n",
    "\n",
    "#FireplaceQu : data description says NA means \"no fireplace\"\n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "\n",
    "#LotFrontage : Since the area of each street connected to the house property most likely \n",
    "#have a similar area to other houses in its neighborhood , \n",
    "#we can fill in missing values by the median LotFrontage of the neighborhood.\n",
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n",
    "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "\n",
    "#GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "\n",
    "#BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : \n",
    "#missing values are likely zero for having no basement\n",
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)    \n",
    "\n",
    "#BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : \n",
    "#For all these categorical basement-related features, NaN means that there is no basement.\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "    \n",
    "#MasVnrArea and MasVnrType : \n",
    "#NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n",
    "\n",
    "#MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "\n",
    "#Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, \n",
    "#this feature won't help in predictive modelling. We can then safely remove it.\n",
    "all_data = all_data.drop(['Utilities'], axis=1)\n",
    "\n",
    "#Functional : data description says NA means typical\n",
    "#TODO: fix `fillna(\"Typ\")`\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "\n",
    "#Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "\n",
    "#KitchenQual: Only one NA value, and same as Electrical, \n",
    "#we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "\n",
    "#Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. \n",
    "#We will just substitute in the most common string\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "\n",
    "#SaleType : Fill in again with most frequent which is \"WD\"\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "\n",
    "#MSSubClass : Na most likely means No building class. We can replace missing values with None\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n",
    "\n",
    "#Transforming some numerical variables that are really categorical\n",
    "#MSSubClass=The building class\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "#Changing OverallCond into a categorical variable\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "\n",
    "#Year and month sold are transformed into categorical features.\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "    \n",
    "# Adding total sqfootage feature \n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "from scipy.stats import skew\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    \n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(\"all data size:\",all_data.shape)\n",
    "\n",
    "#the final engineered data\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]  \n",
    "\n",
    "#save engineered data to hdf5 file for reuse. we often need to save the intermediate result to avoid re-computing everything. \n",
    "#moreover, loading hdf5 is much faster than csv\n",
    "train.to_hdf('train.h5', 'data', mode='w', format='table')\n",
    "test.to_hdf('test.h5', 'data', mode='w', format='table')\n",
    "print(os.listdir(\"../working\"))\n",
    "\n",
    "#we can use pd.read_hdf() to reload hdf5 file to dataframe\n",
    "#train = pd.read_hdf('train.h5','data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "1378e57afebf0e85cf9dcc6a63f27666cd219ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'alpha': 0.9, 'coef0': 2.5, 'degree': 2, 'kernel': 'polynomial'},\n",
       " -0.013257996324007924)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tuning hyper parameter of a base model\n",
    "#we take Kernel Ridge as the base model, then use grid search to find the best hyper parameters\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "              'kernel':('linear', 'rbf','polynomial'), \n",
    "              'degree':[1,2,3],\n",
    "              'coef0':[1,1.5,2,2.5]}\n",
    "KRR = KernelRidge()\n",
    "grid_search = GridSearchCV(KRR, parameters, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit( train.values, y_train )  \n",
    "grid_search.best_params_, grid_search.best_score_  \n",
    "#best params: {'alpha': 1, 'coef0': 2.5, 'degree': 2, 'kernel': 'polynomial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "26bdf4d85fc1f5c0db932db0a56e5a18645b22f1"
   },
   "outputs": [],
   "source": [
    "#modeling\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "#define evaluation method for a given model. we use k-fold cross validation on the training set. \n",
    "#the loss function is root mean square between target and prediction\n",
    "#note: train and y_train are feeded as global variables\n",
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "    \n",
    "#base models with hyper parameters already tuned\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=1, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "#ensemble method 1: model averaging\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    # the reason of clone is avoiding affect the original base models\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]  \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([ model.predict(X) for model in self.models_ ])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "\n",
    "#ensemble method 2: staking models\n",
    "class StackedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "e56d6de61c6da0d188e3ce9f84338127cead2d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 0.1115 (0.0074)\n",
      "\n",
      "ElasticNet score: 0.1116 (0.0074)\n",
      "\n",
      "Kernel Ridge score: 0.1149 (0.0070)\n",
      "\n",
      "Gradient Boosting score: 0.1177 (0.0080)\n",
      "\n",
      "Xgboost score: 0.1157 (0.0063)\n",
      "\n",
      "LGBM score: 0.1154 (0.0071)\n",
      "\n",
      "averaged_models score: 0.1086 (0.0068)\n",
      "\n",
      "Stacked models score: 0.1085 (0.0074)\n"
     ]
    }
   ],
   "source": [
    "#compare the models\n",
    "\n",
    "#first reload the data\n",
    "train = pd.read_hdf('../working/train.h5','data')\n",
    "test = pd.read_hdf('../working/test.h5','data')\n",
    "\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "averaged_models = AveragingModels(models = (lasso, ENet, KRR, GBoost, model_xgb, model_lgb))\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\"averaged_models score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "stacked_models = StackedModels(base_models = (ENet,GBoost, KRR), meta_model = lasso)\n",
    "score = rmsle_cv(stacked_models)\n",
    "print(\"Stacked models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "#results\n",
    "#Lasso score: 0.1115 (0.0074)\n",
    "#ElasticNet score: 0.1116 (0.0074)\n",
    "#Kernel Ridge score: 0.1149 (0.0070)\n",
    "#Gradient Boosting score: 0.1177 (0.0080)\n",
    "#Xgboost score: 0.1161 (0.0079)\n",
    "#LGBM score: 0.1156 (0.0075)\n",
    "#averaged_models score: 0.1087 (0.0072)\n",
    "#Stacked models score: 0.1085 (0.0074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "e2792d1e190d88c9c89acc86af75633bf44dac32",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit and generate result\n",
    "#we combined the two ensemble methods and generate the submission\n",
    "averaged_models.fit(train.values, y_train)\n",
    "averaged_models_pred = np.expm1(averaged_models.predict(test.values))\n",
    "stacked_models.fit(train.values, y_train)\n",
    "stacked_models_pred = np.expm1(stacked_models.predict(test.values))\n",
    "model_xgb.fit(train, y_train)\n",
    "model_xgb_pred = np.expm1(model_xgb.predict(test))\n",
    "model_lgb.fit(train, y_train)\n",
    "model_lgb_pred = np.expm1(model_lgb.predict(test))\n",
    "\n",
    "ensemble = stacked_models_pred * 0.7 + averaged_models_pred * 0.3\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = ensemble\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
